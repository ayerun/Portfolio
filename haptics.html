<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>VR Haptics</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
        <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
        </script>
        <script>
        window.MathJax = {
            tex: {
              tags: 'ams'
            }
          };
        </script>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Projects</a></li>
							<li><a href="about.html">About</a></li>
							<li><a href="contact.html">Contact</a></li>
							<li><a href="Resume_Kumar.pdf" target="_blank" download>Resume</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/arun-kumar-102197/" target="_blank" class="icon brands alt fa-linkedin"><span class="label">Linkedin</span></a></li>
							<li><a href="https://github.com/ayerun" target="_blank" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">April 2021 - December 2021</span>
									<h1>Virtual Reality Haptics</h1>
                                    <p>Created a haptic feedback system for the Valve Index</p>
                                </header>
                                
                                <div class="image main"><img src="images/SLAM/apollo.jpg" alt="" style="max-width: 80%"/></div>
                                
								<h2>Overview</h2>
								<p> This project is an exploration of force/torque feedback in human-robotic systems. 
									VR prides itself on creating immersive environments. 
									However, in current systems the immersion is only an audio-visual experience. 
									Adding the sense of touch to VR systems can enable many more applications such as telerobotic surgery and something else.
									VR haptic gloves are the first expansion into the tactile domain.
									These gloves can resist finger motion.
									This project is concerned with resisting hand and arm motion.
                                </p>

                                <h2>Initial Design</h2>
								<p>In this project, we worked backwards from the end goal.
									Our goal was to deliver force/torque feedback to user's arm.
									We began by thinking about mechanical systems to could produce significant force/torques on the user's arms. 
									Our plan was to use motor(s) to generate torque and use a linkage system to convert the torque into a force or torque felt on the user's hand.
									The best way to visualize this is by imagining a robotic arm in which the user is holding the end effector.
									Actuating the motors in the robotic arm, will extert torques/forces on the user's hand.
									If this robotic arm is stationary in the room, the VR experience is confined to the arm's workspace.
									If the robotic arm is attached to the user, the VR experience if fully mobile.
									There are two main complications to this design.
									The user must carry the weight of the robot, and 
									the user's body must disperse the reaction forces (the base of the robot will exert forces on the user).
									
									From a software and controls standpoint, the primary constraint is the VR tracking frequency.
									The maximum stiffness of a haptically rendered surface is proportional to the speed of the controller.
									The Valve Index 2, comes with a state of the art tracking system, Lighthouse 2.0. 
									This tracking system provides data at 144 Hz.
									The following is the primary question I researched during this project.
									Given a tracking frequency of 144 Hz, what surfaces can we haptically render?
									The answer to this question is based in both engineering and human psychology.
								</p>
								<h2>Materials</h2>
                                <h4>Hardware Selection</h4>
                                <p> For this project, I am using a Valve Index 2 with the Vive Lighthouse 2.0 tracking system. 
									I chose this setup because the Lighthouse 2.0 provides the fastest tracking on the market.
									The other primary components I needed are motors, encoders, and a motor driver.
									I decided to use the T-motor GL60 Kv25. I chose this motor for the following reasons. 
									It has a Kv of 25 rpm/V so it can deliver high torque. 
									It has a has 14 pole pairs, so rotation will be smooth. This is necessary to create compelling haptics.
									It has a stall current of 4.2 A. This will prevent the motor from overheating quickly.
									It weighs 230g which is reasonable for the heaviest component of the wearable device. 
									For the motor driver, I decided to use the ODrive. 
									This motor driver is open-source, so it can be modified if needed. 
									Also, the ODrive implements field oriented control to smoothly control the motor and create compelling haptics. 
									The encoder I chose is the AS5048A. This encoder has 14-bit resolution and is supported by the ODrive.
                                </p>
								<h4>Software Selection</h4>
                                <p> It was difficult to select the software to use for this project.
									Robots run on Linux whereas VR runs on Windows. 
									Since this project, is at the intersection of robotics and VR it was difficult to decide which one to use.
									I settled on using Linux for the following reasons. 
									This project is primarily a robotic technology exploration not a VR technology exploration.
									The robotics side of this project will be easier to build upon in the future if this foundation is built Linux.
									Once the technology is no longer experimental, it can be ported Windows. 
									The downside to using Linux, is that it is difficult to interact with the VR hardware.
									Currently popular game engines, Unity and Unreal Engine, do not support VR development in Linux.
									I decided to use the OpenXR API directly to interact with the hardware. 
									I stripped down an OpenXR example render pipeline for my own use.
									This project focuses on the sense of touch, so for now I am not rendering to the VR headset. 
									I am only using the VR setup and render pipeline to access the controller tracking data.
								</p>

								<h2>Pipeline</h2>
								<a href="images/Haptics/low_level_haptics_pipeline.png" target="_blank"><img class="image fit" src="images/Haptics/low_level_haptics_pipeline.png" alt="" /></a><br/>
								<div class="align center">
									<p class="image center"><b>Figure 1.</b> General software architecture.
										The C++ application inputs the controller pose and encoder data. 
										The haptic model determines the torque that should be applied to the motor.
										The desired torque is sent to the ODrive.
										The ODrive directly controls the motor.
									</p>
								</div><br/>
								<h4>VR Input</h4>
								<p>The Lighthouse 2.0 samples controller and headset data at 144Hz. 
									The SteamVR runtime converts the data from the Lighthouse into meaningful information.
									The OpenXR program collects both controller poses as well as the headset pose, but I am only using a single controller pose. <br/><br/>
									<b>Note on SteamVR Runtime:</b> This is currently the only runtime that can interact with the Lighthouse 2.0. 
									SteamVR is closed source, and their developement practices (specifically for Linux) are questionable. 
									For this project, I am using SteamVR version 1.15.12. Also, I disabled SteamVR from connecting to the internet.
									This is the only way to prevent auto updates. 
									I recommend using Monado, an open-source runtime, when Lighthouse 2.0 is supported.
								</p>

								<h4>C++ Application</h4>
								<p><b>OpenXR Program</b><br/>
									This part of my application was derived from the hello_xr OpenXR example. 
									I simplified this code to only the portions needed for this project. 
									I also added functionality to query the state of the program and retreive necessary information such as controller pose.<br/>
									<b>Haptic Model</b><br/>
									The haptic model uses the controller pose and encoder data to calculate the torque that should be applied to the motor. 
									The code is structured to make the haptic model easily modifiable. 
									This allows developers to easily experiment when attempting to render various surfaces and textures.<br/>
									<b>Motor Communication</b><br/>
									The ODrive comes with a Python-based CLI that serially communicates with the motor driver. 
									Since my application requires C++ for OpenXR, I implemented a motor communication library in C++ enabling direct communication with the ODrive.
								</p>

								<h4>Motor Control</h4>
								<p>After the C++ application sends a desired torque to the ODrive, the ODrive's control algorithms should handle the direct motor control. 
									However, I had to modify the hardware and software on the ODrive to enable the torque control mode.
									The ODrive firmware classifies motors into two categories, high current motors and low current motors. 
									The T-Motor GL60 Kv25 is classified as a low current motor. 
									The ODrive does not implement torque control for low current motors because it can overheat the ODrive and/or the motor. 
									To work around this, I replaced the standard shunt resistors on the ODrive with 0.02Ω resistors. 
									I also modified the shunt resistance value in the firmware to reflect this change.
								</p>

								<h2>1 Degree of Freedom</h2>
								<p>Since this project is novel in the domain of VR haptics, I started with a single degree of freedom system. 
									There were two goals during this stage of the project. The first was to implement the software pipeline previous mentioned.
									The second goal was to assess the ability to haptically render surfaces using encoder data and using VR tracking data. 
									The plan was to haptically render a wall using the model of a spring. Hooke's Law states: $$F_s = -kx$$
									\(F_s\) is the spring force [N]<br/>
									k is the spring constant [N/m]<br/>
									x is the displacement [m]<br/><br/>
									My system only produces torque, so I modified the equation.
									$$T_m = -k&theta;$$
									\(T_m\) is the motor torque [Nm]<br/>
									k is the spring constant [\(\frac{Nm}{degrees}\)]<br/>
									\(&theta;\) is the displacement [degrees]<br/><br/>
								</p>
								<h4>Encoder Feedback</h4>
								<p>The C++ application received encoder data at a frequency of 490 Hz. 
									Using only the encoder feedback I was able to render walls up to a stiffness of ... before instability occurred
								</p>
								<!-- gif of encoder feedback stability and instability -->

								<h4>Tracking Feedback</h4>
								<p> The C++ application received VR controller pose data at a frequency of 144 Hz.
									Using only the pose data, I was able to render walls up to a stiffness of ... before instability occurred </p>
                                </p>
								<!-- gif of tracking feedback stability and instability -->

                                <h2>2 Degrees of Freedom</h2>
								<p>The 1 degree of freedom system confirmed that compelling haptics are feasible given the bottleneck frequency of 144 Hz. 
									The next step was to incorporate more freedom of movement. 
									I put the 1 degree of freedom system on a vertical slider to make a 2 degree of freedom system.
									Haptics in the 2 degree of freedom system cannot be done with just the encoder. 
									The VR tracking data is necessary to determine the position of the controller.
									The VR tracking data or the encoder data could be used to determine the angle of the controller. 
									In this system, I decided to haptically render a floor. 
									You can imagine the VR controller as a sword (extending past the top of the physical controller), and the sword tip (cursor) is interacting with the floor.
									As the cursor touches the floor, the motor delivers a torque to the user that attempts to prevent the sword from penetrating the floor.
									I used the following equation to model the floor: 
									$$T_m = -kx$$
									\(T_m\) is the motor torque [Nm]<br/>
									k is the spring constant [\(\frac{N}{m}\)]<br/>
									x is the depth of the cursor into the floor squared  [\(m^2\)]<br/><br/>
									This equation allowed me to express the spring constant in traditional units.
								</p>
								<!-- gif a instability before filtering -->

								<h4>Filtering</h4>
								<p>Due to the noise in the tracking data, I decided to filter the controller pose. 
									I used the following exponential smoothing function to lowpass filter the controller position:
									$$s_0 = x_0$$
									$$s_n = &alpha;x_n+(1-&alpha;)s_{n-1}$$
									s is the filtered output<br/>
									x is the unfiltered input<br/>
									\(&alpha;\) is the smoothing factor<br/><br/>
									I experimentally and qualitatively determined \(&alpha;=0.5\) to be the optimal smoothing factor. 
								</p>
								<!-- gif of stability after filtering  -->

                                <h2>6 Degrees of Freedom</h2>
								<p>The next step was to detach the controller from the vertical rail. 
									In order to do this, the system needed to become a wearable. 
									My advisor, Bill Strong, designed and manufactured an arm brace for haptic feedback. 
									The arm brace disperses motor reaction forces through the user's forearm.
									This wearable allows the user the move the controller in 3D space and rotate about a single axis (4 degrees of freedom).
									The user can rotate about the other two axes by moving their body about the controller. 
								</p>
								<!-- pic or arm brace -->

								<h4>Linkage</h4>
								<p>A notable feature of the haptic device, is the four bar linkage that attaches the motor to the VR controller. 
									This linkage provides room for the controller to move vertically without rotating the motor. 
									However, controller rotation requires motor rotation and vice versa.
									Therefore, the haptic controls are not significantly affected.
									The lever arm between the motor and controller is longer, so the user feels slightly less torque.
									But, the longer lever arm mimics the human body because the motor is alligned with the user's wrist.
									This linkage makes the wearable feel comfortable and natural even though the user can only easily utilize 4 of the typical 6 degrees of freedoms.
								</p>
								<!-- gif of linkage -->

								<h4>Initial Tests</h4>
								<!-- 6 dof gif no drum -->
								<p>The software for 2 degree of freedom could be used for the new 6 degree of freedom system, if you make two simplifications. 
									One, the floor has infinite length and width. 
									Two, do not rotate the controller about the axis in line with your arm.
									This allowed me to test the wearable without modifying the software. 
									When using the haptic device, it felt almost exactly like a drum.
									I decided to focus on making a virtual drumming demonstration with haptics.
									Haptics projects are difficult to portray in videos, so hopefully the example of a drum to help the viewers of this post potentially understand the feeling of the device and some of its applications.
								</p>

								<h4>Drumkit Haptic Model</h4>
								<p>After testing the haptic device, I started to modify the software to fully utilize all 6 degrees of freedom.
									In the 2 degree of freedom system, tracking the cursor (or drumstick tip) was simple trigonometry.
									All you needed to know was the controller height and angle.
									In the 6 degree of freedom system, tracking the cursor is more complicated. 
									I set up transformation matricies to define a world frame and track the controller and cursor frames.
									I defined the world frame using the initial controller pose when the program starts. 
									The controller is then tracked in reference to the world frame. 
									The cursor is defined as a static transformation from the controller frame. 
									Therefore, the controller and cursor move together.
									The final step was to define drum surfaces.
									I created a drum object in C++.
									The rectangular volume of each drum is defined using a center coordinate (x,y,z), a lenth, and a width.
									The surface starts at the center z coordinated, and extends downward infinitely.
									Each drum surface has an associated spring constant to simulate different drums.
									The drum objects also contain methods and parameters that are used for drum systhesis.
                                </p>

								<h4>Drum Synthesis</h4>
								<p>In order to make a virtual drum demonstration, I needed to synthesize a drum sound.
									To do this, I used Pure Data. 
									Pure Data is an open-source visual programming language that specializes in audio digitial signal processing.
									In Pure Data, I took used samples of various drums and played the sample everytime the cursor contacted the drum. 
									To make the synthesizer more realistic, I multiplied the sampled drum sound with an ADSR (attack, decay, sustain, release) envelope.
									I used parameters from my application to modify the envelope.
									The cursor velocity is used to amplify the drum sound.
									The cursor/drum contact point determins the sustain of the drum sound.
									For example, if you hit the drum as hard as you can directly at the center point, the drum sound is loud and sustains for a long time.
									I implemented a simple communication protocol and utilized a TCP port to send data from the C++ application to the drum synthesizer.
                                </p>

								<h2>Conclusion & Next Steps</h2>
								<p>This project is a succesful first step into VR haptics beyond the fingertips.
									Going back to the initial research question, "Given a tracking frequency of 144 Hz, what surfaces can be haptically render?"
									For wrist haptics, there are many possibilites for haptic rendering at this frequency.
									In my opinion, the controller instability became noticeable at stiffness values greater than 700 \(\frac{N}{m}\).
									At this stiffness, a virtual wall feel realistic to the cautious user.
									Rendering a wall in the headset could discourage users from testing the stiffness of the wall.
									While 700 \(\frac{N}{m}\) is the max stiffness I achieved, there are methods that could potential increase the stiffness.
									One feature I did not get to implement is concurrency in the system.
									The application can receive data from the encoder at 490 Hz, but in my implementation the frequency is capped at 144 Hz because of the render loop.
									By obtaining encoder data in a separate thread, the controller tracking frequency can be aftifically increased via sensor fusion.
									Damping makes surfaces feel less elastic, if that is the desired effect. 
									In my experimentation, the cursor velocity needs aggressive filtering for a compelling damping effect.
									Textures are also possible. For example, the act of drawing the blinds exerts small forces on your wrist that can be simulated.
								</p>
								<p>This project also provided insights on how to proceed for full arm haptic feedback.
									One thing I learned from the arm brace, is that bearing the weight of a robotic arm with is not feasible.
									The arm brace is comfortable, but the weight of the brace alone becomes a nuisance after 10-15 minutes. 
									When designing full arm haptic devices, it would be best to apply the force from a distance. 
									In this case, another part of your body such as your hip could bear the weight of the heavy components as your arm receives the force feedback.
									Moving forward, I no longer have access to the hardware necessary for this project, but hopefully this post provides insights for future developers. 
								</p>

                                <h1><a href="https://github.com/ayerun/SLAM_From_Scratch" target="_blank">-View Full Source Code-</a></h1>
							</section>

					</div>

				<!-- Copyright -->
					<div id="copyright">
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>